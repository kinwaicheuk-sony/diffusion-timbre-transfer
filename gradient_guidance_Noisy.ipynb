{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference example of timbre transfer with dual latent diffusion bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the main tssb folder to the system path, or navigate (cd) to that directory to ensure modules can be imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from main.module_base_latent_cond import (\n",
    "    Model, \n",
    "    AudioDiffusionModel,\n",
    ")\n",
    "\n",
    "\n",
    "from audio_diffusion_pytorch import (\n",
    "    KarrasSamplerReverse, \n",
    "    KarrasSampler,\n",
    "    KarrasSampler_grad_guided,\n",
    "    # KarrasSampler_grad_guidedv2,\n",
    "    KarrasSchedule,\n",
    "    KDistribution,\n",
    "    # PitchTracker,\n",
    "    NormalizedEncodec,\n",
    "    plot_spec, \n",
    "    play_audio\n",
    ")\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import HTML\n",
    "from typing import Dict, Any\n",
    "from torchaudio.prototype.transforms import ChromaSpectrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Distribution, Schedule, and Samplers\n",
    "\n",
    "- **`sigma_min`**, **`sigma_max`**, and **`rho`** will define the **`diffusion_sigma_distribution`**, creating a distribution of sigmas following **Formula 1** from the reference paper.\n",
    "\n",
    "- **`diffusion_schedule`** follows the Karras scheduling algorithm to manage the progression of sigma values during the diffusion process.\n",
    "\n",
    "The following samplers implement the Karras sampling algorithm:\n",
    "- **`diffusion_sampler_reverse`**: Handles the reverse diffusion process (adding noise to audio).\n",
    "- **`diffusion_sampler`**: Handles the forward diffusion process (denoising from noise).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_min = 0.001\n",
    "sigma_max = 100\n",
    "rho = 9.0\n",
    "\n",
    "diffusion_sigma_distribution = KDistribution(sigma_min = sigma_min, sigma_max = sigma_min, rho = rho)\n",
    "diffusion_schedule = KarrasSchedule(sigma_min=sigma_min, sigma_max=sigma_max, rho=rho)  \n",
    "\n",
    "diffusion_sampler_reverse = KarrasSamplerReverse()\n",
    "diffusion_sampler = KarrasSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Loading the Diffusion and PyTorch Lightning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading violin model\n",
    "diffusion_model_violin = AudioDiffusionModel(\n",
    "    diffusion_sigma_distribution =  diffusion_sigma_distribution,\n",
    ")\n",
    "\n",
    "violin_mean_path  = '/workspace/data/kinwai/diffusion-timbre-transfer/ckpts/mean_tensor_enc_violin.pt'\n",
    "violin_std_path = '/workspace/data/kinwai/diffusion-timbre-transfer/ckpts/std_tensor_enc_violin.pt'\n",
    "violin_model_weights = '/workspace/data/kinwai/diffusion-timbre-transfer/ckpts/violin.ckpt'\n",
    "\n",
    "# Pytorch lighting model\n",
    "pl_model_violin = Model(\n",
    "    model = diffusion_model_violin,\n",
    "    mean_path = violin_mean_path,\n",
    "    std_path = violin_std_path,\n",
    ")\n",
    "\n",
    "ckpt_violin = torch.load(violin_model_weights, map_location=device)    \n",
    "pl_model_violin.load_state_dict(ckpt_violin[\"state_dict\"], strict=True)\n",
    "pl_model_violin.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Audio and Converting to Encodec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 24000\n",
    "audio_path = '/workspace/data/kinwai/diffusion-timbre-transfer/audios/216001_1_violin.wav'\n",
    "waveform_raw, orig_sr = torchaudio.load(audio_path)\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=sr)\n",
    "waveform_raw = resampler(waveform_raw)\n",
    "waveform_ori = waveform_raw.unsqueeze(0)\n",
    "waveform_ori = waveform_ori.to(device)\n",
    "\n",
    "# Adjust the audio length to exactly 17 seconds with a sampling rate of 24,000 Hz, either by padding or cropping as needed.\n",
    "pad_size = 409600 - waveform_ori.shape[-1] \n",
    "waveform_ori = torch.nn.functional.pad(waveform_ori, (0, pad_size))\n",
    "\n",
    "print(f'Input waveform shape: {waveform_ori.shape}')\n",
    "plot_spec(waveform_raw.numpy(), sr, title='Real Violin')\n",
    "play_audio(waveform_raw.numpy(), sr)\n",
    "\n",
    "# Conver the input audio to encodec embeddings\n",
    "encodec = NormalizedEncodec(device=device)\n",
    "embeddings_violin = encodec.encode_latent(waveform_ori, pl_model_violin.mean, pl_model_violin.std)\n",
    "print(f'Input Encodec embeddings shape: {embeddings_violin.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise adding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply noise to the input audio using violin model, progressing through the forward diffusion steps to transform it into the shared latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "# Convert to Noise\n",
    "noisy_violin_embeddings = []\n",
    "# produce 4 noisy samples\n",
    "for i in range(4):\n",
    "    noisy_violin_embeddings.append(\n",
    "        pl_model_violin.model.sample(\n",
    "            noise=embeddings_violin,\n",
    "            sampler=diffusion_sampler_reverse,\n",
    "            sigma_schedule=diffusion_schedule,\n",
    "            num_steps=num_steps,\n",
    "        ).cpu().detach()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_waveform = encodec.decode_latent(noisy_violin_embeddings[0].to(device), pl_model_violin.mean, pl_model_violin.std)\n",
    "noise_waveform = noise_waveform.cpu().detach().squeeze(0).numpy()\n",
    "plot_spec(noise_waveform, sr, title='Noisy violin')\n",
    "play_audio(noise_waveform, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using control function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading another audio\n",
    "\n",
    "audio_path = '/workspace/data/kinwai/diffusion-timbre-transfer/audios/216002_1_violin.wav'\n",
    "waveform_raw, orig_sr = torchaudio.load(audio_path)\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=sr)\n",
    "waveform_raw = resampler(waveform_raw)\n",
    "waveform_cond = waveform_raw.unsqueeze(0)\n",
    "waveform_cond = waveform_cond.to(device)\n",
    "\n",
    "# Adjust the audio length to exactly 17 seconds with a sampling rate of 24,000 Hz, either by padding or cropping as needed.\n",
    "pad_size = 409600 - waveform_cond.shape[-1] \n",
    "waveform_cond = torch.nn.functional.pad(waveform_cond, (0, pad_size))\n",
    "\n",
    "print(f'Input waveform shape: {waveform_cond.shape}')\n",
    "plot_spec(waveform_raw.numpy(), sr, title='Real Violin')\n",
    "play_audio(waveform_raw.numpy(), sr)\n",
    "\n",
    "# define control functions f\n",
    "# need to pass the reference condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in [0, 0.1, .3, .5, .7, .9, 1]:\n",
    "    f_chroma = ChromaSpectrogram(sample_rate=sr, n_fft=2048).to(device)\n",
    "    diffusion_sampler_grad_guided = KarrasSampler_grad_guided(\n",
    "        f=f_chroma,\n",
    "        c=waveform_cond, # need to be in raw waveform\n",
    "        encodec=encodec,\n",
    "        mean=pl_model_violin.mean,\n",
    "        std=pl_model_violin.std,\n",
    "        alpha=alpha\n",
    "        )\n",
    "    output_samples = []\n",
    "\n",
    "    for i in range(4):\n",
    "        generated_violin_embeddings = pl_model_violin.model.sample(\n",
    "            noise=noisy_violin_embeddings[i].to(device),\n",
    "            sampler=diffusion_sampler_grad_guided,\n",
    "            sigma_schedule=diffusion_schedule,\n",
    "            num_steps=num_steps,\n",
    "            index=i\n",
    "        ).cpu().detach()\n",
    "        output_samples.append(generated_violin_embeddings)\n",
    "\n",
    "    # for generated_violin_embedding in output_samples:\n",
    "    #     print(generated_violin_embedding.mean())\n",
    "    #     waveform_recon = encodec.decode_latent(generated_violin_embedding.to(device), pl_model_violin.mean, pl_model_violin.std)\n",
    "    #     waveform_recon = waveform_recon.cpu().detach().squeeze(0).numpy()\n",
    "    #     plot_spec(waveform_recon, sr, title='Generated Violin')\n",
    "    #     play_audio(waveform_recon, sr)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
