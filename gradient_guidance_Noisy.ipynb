{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference example of timbre transfer with dual latent diffusion bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the main tssb folder to the system path, or navigate (cd) to that directory to ensure modules can be imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from main.module_base_latent_cond import (\n",
    "    Model, \n",
    "    AudioDiffusionModel,\n",
    ")\n",
    "\n",
    "\n",
    "from audio_diffusion_pytorch import (\n",
    "    KarrasSamplerReverse, \n",
    "    KarrasSampler,\n",
    "    KarrasSampler_grad_guided,\n",
    "    KarrasSampler_grad_guided_alpha_schedule,\n",
    "    KarrasSampler_grad_guided_fixed_binary_pitch,\n",
    "    KarrasSampler_grad_guided_binary,\n",
    "    KarrasSchedule,\n",
    "    KDistribution,\n",
    "    # PitchTracker,\n",
    "    NormalizedEncodec,\n",
    "    plot_spec, \n",
    "    play_audio\n",
    ")\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import HTML\n",
    "from typing import Dict, Any\n",
    "from torchaudio.prototype.transforms import ChromaSpectrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# loading audio1\n",
    "def loading_guidance(path):\n",
    "    audio_path = '/workspace/data/kinwai/diffusion-timbre-transfer/audios/216002_1_violin.wav'\n",
    "    waveform_raw, orig_sr = torchaudio.load(path)\n",
    "    if waveform_raw.shape[0] > 1: # convert stereo to mono\n",
    "        waveform_raw = waveform_raw.mean(dim=0)\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=sr)\n",
    "    waveform_raw = resampler(waveform_raw)\n",
    "    waveform_cond = waveform_raw.unsqueeze(0)\n",
    "    waveform_cond = waveform_cond.to(device)\n",
    "\n",
    "    # Adjust the audio length to exactly 17 seconds with a sampling rate of 24,000 Hz, either by padding or cropping as needed.\n",
    "    pad_size = 409600 - waveform_cond.shape[-1] \n",
    "    waveform_cond = torch.nn.functional.pad(waveform_cond, (0, pad_size))\n",
    "\n",
    "    print(f'Input waveform shape: {waveform_cond.shape}')\n",
    "    plot_spec(waveform_raw.numpy(), sr, title='Real Violin')\n",
    "    play_audio(waveform_raw.numpy(), sr)\n",
    "\n",
    "    return waveform_cond\n",
    "# define control functions f\n",
    "# need to pass the reference condition\n",
    "\n",
    "\n",
    "# auto reload jupyter notebook modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Distribution, Schedule, and Samplers\n",
    "\n",
    "- **`sigma_min`**, **`sigma_max`**, and **`rho`** will define the **`diffusion_sigma_distribution`**, creating a distribution of sigmas following **Formula 1** from the reference paper.\n",
    "\n",
    "- **`diffusion_schedule`** follows the Karras scheduling algorithm to manage the progression of sigma values during the diffusion process.\n",
    "\n",
    "The following samplers implement the Karras sampling algorithm:\n",
    "- **`diffusion_sampler_reverse`**: Handles the reverse diffusion process (adding noise to audio).\n",
    "- **`diffusion_sampler`**: Handles the forward diffusion process (denoising from noise).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_min = 0.001\n",
    "sigma_max = 100\n",
    "rho = 9.0\n",
    "\n",
    "diffusion_sigma_distribution = KDistribution(sigma_min = sigma_min, sigma_max = sigma_min, rho = rho)\n",
    "diffusion_schedule = KarrasSchedule(sigma_min=sigma_min, sigma_max=sigma_max, rho=rho)  \n",
    "\n",
    "diffusion_sampler_reverse = KarrasSamplerReverse()\n",
    "diffusion_sampler = KarrasSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Loading the Diffusion and PyTorch Lightning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading violin model\n",
    "diffusion_model_violin = AudioDiffusionModel(\n",
    "    diffusion_sigma_distribution =  diffusion_sigma_distribution,\n",
    ")\n",
    "\n",
    "violin_mean_path  = '/workspace/data/kinwai/diffusion-timbre-transfer/ckpts/mean_tensor_enc_violin.pt'\n",
    "violin_std_path = '/workspace/data/kinwai/diffusion-timbre-transfer/ckpts/std_tensor_enc_violin.pt'\n",
    "violin_model_weights = '/workspace/data/kinwai/diffusion-timbre-transfer/ckpts/violin.ckpt'\n",
    "\n",
    "# Pytorch lighting model\n",
    "pl_model_violin = Model(\n",
    "    model = diffusion_model_violin,\n",
    "    mean_path = violin_mean_path,\n",
    "    std_path = violin_std_path,\n",
    ")\n",
    "\n",
    "ckpt_violin = torch.load(violin_model_weights, map_location=device)    \n",
    "pl_model_violin.load_state_dict(ckpt_violin[\"state_dict\"], strict=True)\n",
    "pl_model_violin.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Audio and Converting to Encodec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 24000\n",
    "audio_path = '/workspace/data/kinwai/diffusion-timbre-transfer/audios/216001_1_violin.wav'\n",
    "waveform_raw, orig_sr = torchaudio.load(audio_path)\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=sr)\n",
    "waveform_raw = resampler(waveform_raw)\n",
    "waveform_ori = waveform_raw.unsqueeze(0)\n",
    "waveform_ori = waveform_ori.to(device)\n",
    "\n",
    "# Adjust the audio length to exactly 17 seconds with a sampling rate of 24,000 Hz, either by padding or cropping as needed.\n",
    "pad_size = 409600 - waveform_ori.shape[-1] \n",
    "waveform_ori = torch.nn.functional.pad(waveform_ori, (0, pad_size))\n",
    "\n",
    "print(f'Input waveform shape: {waveform_ori.shape}')\n",
    "plot_spec(waveform_raw.numpy(), sr, title='Real Violin')\n",
    "play_audio(waveform_raw.numpy(), sr)\n",
    "\n",
    "# Conver the input audio to encodec embeddings\n",
    "encodec = NormalizedEncodec(device=device)\n",
    "embeddings_violin = encodec.encode_latent(waveform_ori, pl_model_violin.mean, pl_model_violin.std)\n",
    "print(f'Input Encodec embeddings shape: {embeddings_violin.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise adding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply noise to the input audio using violin model, progressing through the forward diffusion steps to transform it into the shared latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "# Convert to Noise\n",
    "noisy_violin_embeddings = []\n",
    "# produce 4 noisy samples\n",
    "for i in range(4):\n",
    "    noisy_violin_embeddings.append(\n",
    "        pl_model_violin.model.sample(\n",
    "            noise=embeddings_violin,\n",
    "            sampler=diffusion_sampler_reverse,\n",
    "            sigma_schedule=diffusion_schedule,\n",
    "            num_steps=num_steps,\n",
    "        ).cpu().detach()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_waveform = encodec.decode_latent(noisy_violin_embeddings[0].to(device), pl_model_violin.mean, pl_model_violin.std)\n",
    "noise_waveform = noise_waveform.cpu().detach().squeeze(0).numpy()\n",
    "plot_spec(noise_waveform, sr, title='Noisy violin')\n",
    "play_audio(noise_waveform, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = ['00', '01', '02', '03'] # for .pt to .wav conversion\n",
    "\n",
    "waveform_cond1 = loading_guidance('/workspace/data/kinwai/diffusion-timbre-transfer/audios/216002_1_violin.wav')\n",
    "waveform_cond2 = loading_guidance('/workspace/data/kinwai/diffusion-timbre-transfer/audios/melody_guidance.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $d'_i=d_i + \\alpha\\bar{v}\\|d_i\\|_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0,1] norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromagram Guidance: Another violin clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### constant alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'Noise/0_1norm/chromagram'\n",
    "alphas = [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "for alpha in alphas:\n",
    "    f_chroma = ChromaSpectrogram(sample_rate=sr, n_fft=2048).to(device)\n",
    "    diffusion_sampler_grad_guided = KarrasSampler_grad_guided(\n",
    "        f=f_chroma,\n",
    "        c=waveform_cond1, # need to be in raw waveform\n",
    "        encodec=encodec,\n",
    "        mean=pl_model_violin.mean,\n",
    "        std=pl_model_violin.std,\n",
    "        alpha=alpha,\n",
    "        tag=tag\n",
    "        )\n",
    "    output_samples = []\n",
    "\n",
    "    for i in range(4):\n",
    "        generated_violin_embeddings = pl_model_violin.model.sample(\n",
    "            noise=noisy_violin_embeddings[i].to(device),\n",
    "            sampler=diffusion_sampler_grad_guided,\n",
    "            sigma_schedule=diffusion_schedule,\n",
    "            num_steps=num_steps,\n",
    "            index=i\n",
    "        ).cpu().detach()\n",
    "        output_samples.append(generated_violin_embeddings)\n",
    "\n",
    "    # for generated_violin_embedding in output_samples:\n",
    "    #     print(generated_violin_embedding.mean())\n",
    "    #     waveform_recon = encodec.decode_latent(generated_violin_embedding.to(device), pl_model_violin.mean, pl_model_violin.std)\n",
    "    #     waveform_recon = waveform_recon.cpu().detach().squeeze(0).numpy()\n",
    "    #     plot_spec(waveform_recon, sr, title='Generated Violin')\n",
    "    #     play_audio(waveform_recon, sr)\n",
    "\n",
    "# convert .pt to .wav\n",
    "sample_ids = ['00', '01', '02', '03']\n",
    "for alpha in alphas:\n",
    "    exp = f\"alpha_exp/{tag}/{alpha}\"\n",
    "    for sample_id in sample_ids:\n",
    "        audio_49 = torch.load(f'results/{exp}/{sample_id}_audio_pred_49.pt').cpu()\n",
    "        audio_98 = torch.load(f'results/{exp}/{sample_id}_audio_pred_98.pt').cpu()\n",
    "        # save audio into mp3\n",
    "        torchaudio.save(f'results/{exp}/{sample_id}_audio_pred_49.mp3', audio_49.detach().squeeze(0), sr)\n",
    "        torchaudio.save(f'results/{exp}/{sample_id}_audio_pred_98.mp3', audio_98.detach().squeeze(0), sr)\n",
    "\n",
    "# chroma_pred_49 = torch.load(f'frames/{exp}/{sample_id}_chroma_pred_49.pt').cpu()\n",
    "# chroma_pred_98 = torch.load(f'frames/{exp}/{sample_id}_chroma_pred_98.pt').cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### changing alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'Noise/0_1norm/chromagram'\n",
    "alpha_schedules = ['increase', 'decrease']\n",
    "for alpha_schedule in alpha_schedules:\n",
    "    if alpha_schedule == 'increase':\n",
    "        alpha = torch.linspace(0, 1, num_steps-1)\n",
    "    elif alpha_schedule == 'decrease':\n",
    "        alpha = torch.linspace(1, 0, num_steps-1)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid alpha_schedule: {alpha}')\n",
    "    f_chroma = ChromaSpectrogram(sample_rate=sr, n_fft=2048).to(device)\n",
    "    diffusion_sampler_grad_guided = KarrasSampler_grad_guided_alpha_schedule(\n",
    "        f=f_chroma,\n",
    "        c=waveform_cond1, # need to be in raw waveform\n",
    "        encodec=encodec,\n",
    "        mean=pl_model_violin.mean,\n",
    "        std=pl_model_violin.std,\n",
    "        alpha=alpha,\n",
    "        alpha_schedule=alpha_schedule,\n",
    "        tag=tag\n",
    "        )\n",
    "    output_samples = []\n",
    "    for i in range(4):\n",
    "        generated_violin_embeddings = pl_model_violin.model.sample(\n",
    "            noise=noisy_violin_embeddings[i].to(device),\n",
    "            sampler=diffusion_sampler_grad_guided,\n",
    "            sigma_schedule=diffusion_schedule,\n",
    "            num_steps=num_steps,\n",
    "            index=i\n",
    "        ).cpu().detach()\n",
    "        output_samples.append(generated_violin_embeddings)\n",
    "\n",
    "    # for generated_violin_embedding in output_samples:\n",
    "    #     print(generated_violin_embedding.mean())\n",
    "    #     waveform_recon = encodec.decode_latent(generated_violin_embedding.to(device), pl_model_violin.mean, pl_model_violin.std)\n",
    "    #     waveform_recon = waveform_recon.cpu().detach().squeeze(0).numpy()\n",
    "    #     plot_spec(waveform_recon, sr, title='Generated Violin')\n",
    "    #     play_audio(waveform_recon, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert .pt to .wav\n",
    "sample_ids = ['00', '01', '02', '03']\n",
    "for alpha in alpha_schedules:\n",
    "    exp = f\"alpha_exp/{tag}/{alpha}\"\n",
    "    for sample_id in sample_ids:\n",
    "        audio_49 = torch.load(f'results/{exp}/{sample_id}_audio_pred_49.pt').cpu()\n",
    "        audio_98 = torch.load(f'results/{exp}/{sample_id}_audio_pred_98.pt').cpu()\n",
    "        # save audio into mp3\n",
    "        torchaudio.save(f'results/{exp}/{sample_id}_audio_pred_49.mp3', audio_49.detach().squeeze(0), sr)\n",
    "        torchaudio.save(f'results/{exp}/{sample_id}_audio_pred_98.mp3', audio_98.detach().squeeze(0), sr)\n",
    "\n",
    "# chroma_pred_49 = torch.load(f'frames/{exp}/{sample_id}_chroma_pred_49.pt').cpu()\n",
    "# chroma_pred_98 = torch.load(f'frames/{exp}/{sample_id}_chroma_pred_98.pt').cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromagram Guidance: Another violin clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Original noise adding===\n",
    "# f_chroma = ChromaSpectrogram(sample_rate=sr, n_fft=2048).to(device)\n",
    "# diffusion_sampler_grad_guided = KarrasSampler(\n",
    "#     # f=f_chroma,\n",
    "#     # c=waveform_cond, # need to be in raw waveform\n",
    "#     encodec=encodec,\n",
    "#     mean=pl_model_violin.mean,\n",
    "#     std=pl_model_violin.std,\n",
    "#     # loss_type='bce'\n",
    "#     )\n",
    "# output_samples = []\n",
    "loss_types = ['mse', 'bce']\n",
    "for loss_type in loss_types:\n",
    "    f_chroma = ChromaSpectrogram(sample_rate=sr, n_fft=2048).to(device)\n",
    "    diffusion_sampler_grad_guided = KarrasSampler_grad_guided_binary(\n",
    "        encodec=encodec,\n",
    "        f=f_chroma,\n",
    "        c=waveform_cond1, # need to be in raw waveform\n",
    "        mean=pl_model_violin.mean,\n",
    "        std=pl_model_violin.std,\n",
    "        loss_type=loss_type\n",
    "        )\n",
    "    output_samples = []\n",
    "\n",
    "    for i in range(4):\n",
    "        generated_violin_embeddings = pl_model_violin.model.sample(\n",
    "            noise=noisy_violin_embeddings[i].to(device),\n",
    "            sampler=diffusion_sampler_grad_guided,\n",
    "            sigma_schedule=diffusion_schedule,\n",
    "            num_steps=num_steps,\n",
    "            index=i\n",
    "        ).cpu().detach()\n",
    "        output_samples.append(generated_violin_embeddings)\n",
    "\n",
    "        # for generated_violin_embedding in output_samples:\n",
    "        #     print(generated_violin_embedding.mean())\n",
    "        #     waveform_recon = encodec.decode_latent(generated_violin_embedding.to(device), pl_model_violin.mean, pl_model_violin.std)\n",
    "        #     waveform_recon = waveform_recon.cpu().detach().squeeze(0).numpy()\n",
    "        #     plot_spec(waveform_recon, sr, title='Generated Violin')\n",
    "        #     play_audio(waveform_recon, sr)\n",
    "\n",
    "# convert .pt to .wav\n",
    "sample_ids = ['00', '01', '02', '03']\n",
    "for loss_type in loss_types:\n",
    "    exp = f\"binary/{loss_type}\"\n",
    "    for sample_id in sample_ids:\n",
    "        audio_49 = torch.load(f'results/{exp}/{sample_id}_audio_pred_49.pt').cpu()\n",
    "        audio_98 = torch.load(f'results/{exp}/{sample_id}_audio_pred_98.pt').cpu()\n",
    "        # save audio into mp3\n",
    "        torchaudio.save(f'results/{exp}/{sample_id}_audio_pred_49.mp3', audio_49.detach().squeeze(0), sr)\n",
    "        torchaudio.save(f'results/{exp}/{sample_id}_audio_pred_98.mp3', audio_98.detach().squeeze(0), sr)\n",
    "\n",
    "# chroma_pred_49 = torch.load(f'frames/{exp}/{sample_id}_chroma_pred_49.pt').cpu()\n",
    "# chroma_pred_98 = torch.load(f'frames/{exp}/{sample_id}_chroma_pred_98.pt').cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromagram Guidance: Major Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Original noise adding===\n",
    "# f_chroma = ChromaSpectrogram(sample_rate=sr, n_fft=2048).to(device)\n",
    "# diffusion_sampler_grad_guided = KarrasSampler(\n",
    "#     # f=f_chroma,\n",
    "#     # c=waveform_cond, # need to be in raw waveform\n",
    "#     encodec=encodec,\n",
    "#     mean=pl_model_violin.mean,\n",
    "#     std=pl_model_violin.std,\n",
    "#     # loss_type='bce'\n",
    "#     )\n",
    "# output_samples = []\n",
    "loss_types = ['mse', 'bce']\n",
    "for loss_type in loss_types:\n",
    "    f_chroma = ChromaSpectrogram(sample_rate=sr, n_fft=2048).to(device)\n",
    "    diffusion_sampler_grad_guided = KarrasSampler_grad_guided_fixed_binary_pitch(\n",
    "        encodec=encodec,\n",
    "        f=f_chroma,\n",
    "        c=waveform_cond2, # need to be in raw waveform\n",
    "        mean=pl_model_violin.mean,\n",
    "        std=pl_model_violin.std,\n",
    "        loss_type=loss_type\n",
    "        )\n",
    "    output_samples = []\n",
    "\n",
    "    for i in range(4):\n",
    "        generated_violin_embeddings = pl_model_violin.model.sample(\n",
    "            noise=noisy_violin_embeddings[i].to(device),\n",
    "            sampler=diffusion_sampler_grad_guided,\n",
    "            sigma_schedule=diffusion_schedule,\n",
    "            num_steps=num_steps,\n",
    "            index=i\n",
    "        ).cpu().detach()\n",
    "        output_samples.append(generated_violin_embeddings)\n",
    "\n",
    "        # for generated_violin_embedding in output_samples:\n",
    "        #     print(generated_violin_embedding.mean())\n",
    "        #     waveform_recon = encodec.decode_latent(generated_violin_embedding.to(device), pl_model_violin.mean, pl_model_violin.std)\n",
    "        #     waveform_recon = waveform_recon.cpu().detach().squeeze(0).numpy()\n",
    "        #     plot_spec(waveform_recon, sr, title='Generated Violin')\n",
    "        #     play_audio(waveform_recon, sr)\n",
    "\n",
    "# convert .pt to .wav\n",
    "sample_ids = ['00', '01', '02', '03']\n",
    "for loss_type in loss_types:\n",
    "    exp = f\"binary_major_scale/{loss_type}\"\n",
    "    for sample_id in sample_ids:\n",
    "        audio_49 = torch.load(f'results/{exp}/{sample_id}_audio_pred_49.pt').cpu()\n",
    "        audio_98 = torch.load(f'results/{exp}/{sample_id}_audio_pred_98.pt').cpu()\n",
    "        # save audio into mp3\n",
    "        torchaudio.save(f'results/{exp}/{sample_id}_audio_pred_49.mp3', audio_49.detach().squeeze(0), sr)\n",
    "        torchaudio.save(f'results/{exp}/{sample_id}_audio_pred_98.mp3', audio_98.detach().squeeze(0), sr)\n",
    "\n",
    "# chroma_pred_49 = torch.load(f'frames/{exp}/{sample_id}_chroma_pred_49.pt').cpu()\n",
    "# chroma_pred_98 = torch.load(f'frames/{exp}/{sample_id}_chroma_pred_98.pt').cpu()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
